[package]
name = "LLMInferenceService"
version = "0.1.0"
edition = "2024"

[dependencies]
# Web框架
axum = { version = "0.8.7", features = ["macros"] }
tokio = { version = "1.48.0", features = ["full", "sync"] }
tower-http = { version = "0.6.8", features = ["cors", "trace", "compression-full"] }
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }

# 序列化
serde = { version = "1.0.228", features = ["derive"] }
serde_json = "1.0.145"

# 流式响应
tokio-stream = "0.1.17"
futures = "0.3.31"
async-stream = "0.3.6"

# 错误处理
thiserror = "2.0.17"
anyhow = "1.0"

# ============================================================
# llama-burn 依赖 (取消注释以使用真实模型)
# ============================================================
# 
# 启用 llama-burn 后，需要:
# 1. 取消下面的依赖注释
# 2. 在 src/model.rs 中取消 llama-burn 实现的注释
# 3. 注释掉演示实现
#
llama-burn = {git = "https://github.com/tracel-ai/models",package = "llama-burn", features = ["llama3", "tiny", "pretrained"]}
burn = { version = "0.19.1", features = ["wgpu", "fusion", "vulkan"] }

[[bin]]
name = "server"
path = "src/main.rs"

[profile.release]
opt-level = 3
lto = "thin"

[profile.dev]
opt-level = 1
